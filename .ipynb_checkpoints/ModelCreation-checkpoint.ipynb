{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../data\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_1_DIR = '../data/train/1/'\n",
    "TRAIN_0_DIR = '../data/train/0/'\n",
    "HARD_EXAMPLE_DIR = 'data_save_difficult_nofaces/'\n",
    "\n",
    "\n",
    "ROWS = 36\n",
    "COLS = 36\n",
    "CHANNELS = 1\n",
    "\n",
    "\n",
    "TRIAN_1_PATH = [TRAIN_1_DIR+i for i in os.listdir(TRAIN_1_DIR)]\n",
    "TRIAN_0_PATH = [TRAIN_0_DIR+i for i in os.listdir(TRAIN_0_DIR)]\n",
    "TRIAN_EXAMPLE_PATH = [HARD_EXAMPLE_DIR+i for i in os.listdir(HARD_EXAMPLE_DIR)]\n",
    "\n",
    "\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) #\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, ROWS, COLS), dtype=np.uint8)\n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        data[i] = image\n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))\n",
    "    \n",
    "    return data\n",
    "\n",
    "TRIAN_1_IMAGES = prep_data(TRIAN_1_PATH)\n",
    "TRIAN_0_IMAGES = prep_data(TRIAN_0_PATH)\n",
    "HARD_EXAMPLE_SET = prep_data(TRIAN_EXAMPLE_PATH)\n",
    "HARD_EXAMPLE_SET = []\n",
    "\n",
    "\n",
    "\n",
    "def add_hard_no_face_example(no_face_images_vect):\n",
    "    global HARD_EXAMPLE_SET\n",
    "    HARD_EXAMPLE_SET += no_face_images_vect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(idx):\n",
    "    face = TRIAN_1_IMAGES[idx]\n",
    "    noface = TRIAN_0_IMAGES[idx]\n",
    "    pair = np.concatenate((face, noface), axis=1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(pair, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "show_example(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepar train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TEST_BY_CLASS = 3000\n",
    "def shuffle_and_get_new_train_set():\n",
    "    random.shuffle(TRIAN_1_IMAGES)\n",
    "    random.shuffle(TRIAN_0_IMAGES)\n",
    "    test_images_1 = TRIAN_1_IMAGES[:NB_TEST_BY_CLASS]\n",
    "    test_images_0 = TRIAN_0_IMAGES[:NB_TEST_BY_CLASS]\n",
    "    train_1 = TRIAN_1_IMAGES[NB_TEST_BY_CLASS:]\n",
    "    train_0 = TRIAN_0_IMAGES[NB_TEST_BY_CLASS:]\n",
    "    return train_1, train_0, test_images_1, test_images_0\n",
    "\n",
    "def prepar_train_images():\n",
    "    train_1, train_0, test_images_1, test_images_0 = shuffle_and_get_new_train_set()\n",
    "    train_images = np.array(list(train_1[:(20000+len(HARD_EXAMPLE_SET))]) +\\\n",
    "                            list(train_0[:20000]) + list(HARD_EXAMPLE_SET))\n",
    "    train_images.resize((len(train_images), 36, 36, 1))\n",
    "    train_and_label = list(zip(train_images, ([1]*(len(train_images)//2)) + ([0]*(len(train_images)//2))))\n",
    "    random.shuffle(train_and_label)\n",
    "    train_images = list(map(lambda x: x[0], train_and_label))\n",
    "    train_labels = list(map(lambda x: x[1], train_and_label))\n",
    "    test_imagies = list(test_images_1) + list(test_images_0)\n",
    "    test_imagies = np.array(test_imagies)\n",
    "    test_imagies.resize((NB_TEST_BY_CLASS*2, 36, 36, 1))\n",
    "    return np.array(train_images), np.array(train_labels), test_imagies\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "train_images, train_labels, test_imagies = prepar_train_images()\n",
    "print(len(train_images))\n",
    "print(train_images.shape)\n",
    "print(len(train_labels))\n",
    "print(train_labels.size)\n",
    "print(train_labels[:10])\n",
    "print(len(test_imagies))\n",
    "print(test_imagies.shape)\n",
    "\n",
    "print(train_labels[1])\n",
    "plt.imshow(train_images[1].reshape((36,36)), cmap='gray')\n",
    "plt.show()\n",
    "print(train_labels[2])\n",
    "plt.imshow(train_images[2].reshape((36,36)), cmap='gray')\n",
    "plt.show()\n",
    "print(train_labels[3])\n",
    "plt.imshow(train_images[3].reshape((36,36)), cmap='gray')\n",
    "plt.show()\n",
    "print(train_labels[4])\n",
    "plt.imshow(train_images[4].reshape((36,36)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-4)\n",
    "objective = 'binary_crossentropy'\n",
    "\n",
    "\n",
    "def faceRecognition():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(4, 5, strides=(1,1), border_mode='same',\n",
    "                     input_shape=(36, 36, 1), data_format=\"channels_last\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(16, 3, strides=(1,1), border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss=objective, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = faceRecognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 5\n",
    "batch_size = 32\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('accuracy'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')        \n",
    "        \n",
    "def train_model():\n",
    "    \n",
    "    history = LossHistory()\n",
    "    model.fit(train_images, train_labels, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "              validation_split=0.25, verbose=1, shuffle=True, callbacks=[history, early_stopping])\n",
    "    return history\n",
    "\n",
    "history = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.losses\n",
    "val_loss = history.val_losses\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Face_recog Loss Trend')\n",
    "plt.plot(loss, 'blue', label='Training Loss')\n",
    "plt.plot(val_loss, 'green', label='Validation Loss')\n",
    "plt.xticks(range(0,nb_epoch)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple iteration training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto') \n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCH = 10\n",
    "\n",
    "\n",
    "def addjust_train_set_for_hard_example(predictions, threshold):\n",
    "    hard_0_ex_list = []\n",
    "    nb_easy_ex_added = 0\n",
    "    nb_hard_ex_added = 0\n",
    "    for i in range(NB_TEST_BY_CLASS):\n",
    "        if predictions[NB_TEST_BY_CLASS+i] > threshold:\n",
    "            hard_0_ex_list.append(list(np.array(test_imagies[NB_TEST_BY_CLASS+i]).reshape(36,36)))\n",
    "            nb_hard_ex_added += 1 \n",
    "        else:\n",
    "            if nb_easy_ex_added < nb_hard_ex_added:\n",
    "                hard_0_ex_list.append(list(np.array(test_imagies[NB_TEST_BY_CLASS+i]).reshape(36,36)))\n",
    "                nb_easy_ex_added += 1\n",
    "    add_hard_no_face_example(hard_0_ex_list)\n",
    "\n",
    "def multiple_iteration_train_model(nb_iteration=10):\n",
    "    threshold_step = (0.9-0.5)/nb_iteration\n",
    "    model = None\n",
    "    for i in range(nb_iteration):\n",
    "        sleep(10)\n",
    "        model = faceRecognition()\n",
    "        train_images, train_labels, test_imagies = prepar_train_images()\n",
    "        model.fit(train_images, train_labels, batch_size=BATCH_SIZE, nb_epoch=NB_EPOCH,\n",
    "                  validation_split=0.25, verbose=0, shuffle=True, callbacks=[early_stopping])\n",
    "        predictions = model.predict(test_imagies, verbose=0)\n",
    "        print(len(list(filter(lambda x: x > (0.5+(i*threshold_step)), predictions[NB_TEST_BY_CLASS:]))))\n",
    "        print(len(list(filter(lambda x: x < (0.5+(i*threshold_step)), predictions[:NB_TEST_BY_CLASS]))))\n",
    "        addjust_train_set_for_hard_example(predictions, (0.5+(i*threshold_step)))\n",
    "    return model\n",
    "        \n",
    "        \n",
    "#model = multiple_iteration_train_model(nb_iteration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_imagies, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos = []\n",
    "false_neg = []\n",
    "nb_error = []\n",
    "for tolerance in [0.9, 0.6, 0.5, 0.4, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001, 0]:\n",
    "    false_neg.append(len(list(filter(lambda x: x[0] == x[1], list(zip(([0]*(len(test_imagies)//2)), list(map(lambda x: int(x+tolerance), predictions[:(len(test_imagies)//2)]))))))))\n",
    "    false_pos.append(len(list(filter(lambda x: x[0] == x[1], list(zip(([1]*(len(test_imagies)//2)), list(map(lambda x: int(x+tolerance), predictions[(len(test_imagies)//2)+1:]))))))))\n",
    "    nb_error.append((len(list(filter(lambda x: x[0] == x[1], list(zip(([0]*(len(test_imagies)//2)) +\n",
    "                                                                        [1]*(len(test_imagies)//2),\n",
    "                                       list(map(lambda x: int(x+tolerance), predictions))))))), tolerance))\n",
    "print(false_pos)\n",
    "print(false_neg)\n",
    "print(\"nb error optimum : \" + str(min(list(map(lambda x: x[0], nb_error)))) + \" for tolereance of : \" +\n",
    "     str(list(map(lambda x: x[1], list(filter(lambda x: x[0] == min(list(map(lambda x: x[0], nb_error))), nb_error))))))\n",
    "\n",
    "false_pos_rate = list(map(lambda x: x/max(false_pos), false_pos))\n",
    "true_pos_rate = list(map(lambda x: (1-(x/max(false_neg))), false_neg))\n",
    "\n",
    "plt.plot(false_pos_rate, true_pos_rate)\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del model  # deletes the existing model\n",
    "\n",
    "model = load_model('my_model.h5')\n",
    "predictions = model.predict(test_imagies, verbose=0)\n",
    "\n",
    "false_pos = []\n",
    "false_neg = []\n",
    "nb_error = []\n",
    "for tolerance in [0.9, 0.6, 0.5, 0.4, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001, 0]:\n",
    "    false_neg.append(len(list(filter(lambda x: x[0] == x[1], list(zip(([0]*(len(test_imagies)//2)), list(map(lambda x: int(x+tolerance), predictions[:(len(test_imagies)//2)]))))))))\n",
    "    false_pos.append(len(list(filter(lambda x: x[0] == x[1], list(zip(([1]*(len(test_imagies)//2)), list(map(lambda x: int(x+tolerance), predictions[(len(test_imagies)//2)+1:]))))))))\n",
    "    nb_error.append((len(list(filter(lambda x: x[0] == x[1], list(zip(([0]*(len(test_imagies)//2)) +\n",
    "                                                                        [1]*(len(test_imagies)//2),\n",
    "                                       list(map(lambda x: int(x+tolerance), predictions))))))), tolerance))\n",
    "print(false_pos)\n",
    "print(false_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
